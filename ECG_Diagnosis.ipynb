{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ECG Diagnosis Code**\n",
    "\n",
    "This code is based on the code developed here: https://doi.org/10.1038/s41467-020-15432-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n",
    "                                        CSVLogger, EarlyStopping)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Load in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in test data\n",
    "path_to_hdf5 = cwd + '\\\\data\\\\test\\\\ecg_tracings.hdf5'\n",
    "dataset_name = 'tracings'\n",
    "path_to_csv = cwd + '\\\\data\\\\test\\\\gold_standard.csv'\n",
    "\n",
    "#Order is based on test set csv\n",
    "abnormalities = ['1dAVb','RBBB', 'LBBB', 'SB',  'AF', 'ST']\n",
    "\n",
    "labels_test = pd.read_csv(path_to_csv).values\n",
    "f = h5py.File(path_to_hdf5, \"r\")\n",
    "tracings_test = f[dataset_name][()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next find the class percentages in the test data since we know it was properly balanced\n",
    "\n",
    "This can be used to select the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPercents(labels:np.ndarray, average: bool=True):\n",
    "    bias = []\n",
    "    for ii in range(labels.shape[-1]):\n",
    "        bias.append(np.sum(labels[:,ii])/len(labels[:,ii]))\n",
    "    \n",
    "    if average:\n",
    "        bias = np.mean(bias)\n",
    "        return bias\n",
    "    else:\n",
    "        return np.array(bias)\n",
    "\n",
    "test_per = FindPercents(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now due to memory constraints can only take 3 exams from the 15% of the train set\n",
    "\n",
    "Therefore I will take the 3 exams with the average percents closest to the balanced test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File: 0\n",
      "Loading File: 1\n",
      "Loading File: 2\n",
      "Loading File: 3\n",
      "Loading File: 4\n",
      "Loading File: 5\n",
      "Loading File: 6\n",
      "Loading File: 7\n",
      "Loading File: 8\n",
      "Loading File: 9\n",
      "Loading File: 10\n",
      "Loading File: 11\n",
      "Loading File: 12\n",
      "Loading File: 13\n",
      "Loading File: 14\n",
      "Loading File: 15\n",
      "Loading File: 16\n",
      "The best exams are: [9, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "def ProcessLabels(data_info: pd.DataFrame, train_ids: np.ndarray, abnormalities: list):\n",
    "    good_rows = []\n",
    "    bad_rows = []\n",
    "    labels = np.array([])\n",
    "    for ii, id in enumerate(train_ids):\n",
    "        if id in data_info['exam_id'].to_numpy():\n",
    "            good_rows.append(ii)\n",
    "            data_row = data_info.iloc[np.where(data_info['exam_id'].to_numpy() == id)[0][0]]\n",
    "            labels = np.append(labels, data_row[abnormalities].values.tolist())\n",
    "        else:\n",
    "            bad_rows.append(ii)\n",
    "\n",
    "    labels = np.reshape(labels, (len(good_rows), len(abnormalities)))\n",
    "    \n",
    "    return labels.astype(int), np.array(good_rows)\n",
    "\n",
    "#Load in training data - can only take 3 due to memory constraints so use the best 3\n",
    "path_to_hdf5s = cwd + '\\\\data\\\\train\\\\exams\\\\'\n",
    "path_to_csv = cwd + '\\\\data\\\\train\\\\exams.csv'\n",
    "data_info = pd.read_csv(path_to_csv)\n",
    "all_exams = os.listdir(path_to_hdf5s)\n",
    "\n",
    "#Process the exams data\n",
    "for ii, path_to_hdf5 in enumerate(all_exams):\n",
    "    print('Loading File: {}'.format(ii))\n",
    "    f = h5py.File(path_to_hdf5s + path_to_hdf5, \"r\")\n",
    "    train_ids = f['exam_id'][()]\n",
    "    f.close()\n",
    "    exam_labels, good_rows = ProcessLabels(data_info, train_ids, abnormalities)\n",
    "    \n",
    "    #Find best exams\n",
    "    if ii == 0:\n",
    "        scores = np.array([np.abs(test_per-FindPercents(exam_labels))])\n",
    "    else:\n",
    "        scores = np.append(scores, np.abs(test_per-FindPercents(exam_labels)))\n",
    "\n",
    "#Find the best exams\n",
    "best_exams = []\n",
    "for ii in range(3):\n",
    "    best_exams.append(np.argmin(scores))\n",
    "    scores = np.delete(scores,np.argmin(scores))\n",
    "\n",
    "print('The best exams are: {}'.format(best_exams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After find the best exams, load only these exams for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File: 0\n",
      "Loading File: 1\n",
      "Loading File: 2\n"
     ]
    }
   ],
   "source": [
    "#Load in training data \n",
    "path_to_hdf5s = cwd + '\\\\data\\\\train\\\\Best_Exams\\\\'\n",
    "path_to_csv = cwd + '\\\\data\\\\train\\\\exams.csv'\n",
    "data_info = pd.read_csv(path_to_csv)\n",
    "all_exams = os.listdir(path_to_hdf5s)\n",
    "\n",
    "#Process the exams data\n",
    "for ii, path_to_hdf5 in enumerate(all_exams):\n",
    "    print('Loading File: {}'.format(ii))\n",
    "    f = h5py.File(path_to_hdf5s + path_to_hdf5, \"r\")\n",
    "    exam_tracings = f['tracings'][()]\n",
    "    train_ids = f['exam_id'][()]\n",
    "    f.close()\n",
    "    exam_labels, good_rows = ProcessLabels(data_info, train_ids, abnormalities)\n",
    "\n",
    "    if ii == 0:\n",
    "        tracings_train = exam_tracings[good_rows].astype(np.float16)\n",
    "        labels_train = exam_labels\n",
    "    else:\n",
    "        tracings_train = np.append(tracings_train, exam_tracings[good_rows].astype(np.float16), axis = 0)\n",
    "        labels_train = np.append(labels_train, exam_labels, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Data Functions**\n",
    "\n",
    "A function for saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveObject(object_to_save, save_path):\n",
    "    '''Load path must have the file name with the .pickle extension'''\n",
    "    pickle_out = open(save_path, 'wb')\n",
    "    pickle.dump(object_to_save, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadObject(load_path):\n",
    "    infile = open(load_path, 'rb')\n",
    "    Loaded_object = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    return Loaded_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the NN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(object):\n",
    "    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n",
    "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "                 postactivation_bn=False, activation_function='relu'):\n",
    "        self.n_samples_out = n_samples_out\n",
    "        self.n_filters_out = n_filters_out\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.dropout_rate = 1 - dropout_keep_prob\n",
    "        self.kernel_size = kernel_size\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _skip_connection(self, y, downsample, n_filters_in):\n",
    "        \"\"\"Implement skip connection.\"\"\"\n",
    "        # Deal with downsampling\n",
    "        if downsample > 1:\n",
    "            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n",
    "        elif downsample == 1:\n",
    "            y = y\n",
    "        else:\n",
    "            raise ValueError(\"Number of samples should always decrease.\")\n",
    "        # Deal with n_filters dimension increase\n",
    "        if n_filters_in != self.n_filters_out:\n",
    "            # This is one of the two alternatives presented in ResNet paper\n",
    "            # Other option is to just fill the matrix with zeros.\n",
    "            y = Conv1D(self.n_filters_out, 1, padding='same',\n",
    "                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n",
    "        return y\n",
    "\n",
    "    def _batch_norm_plus_activation(self, x):\n",
    "        if self.postactivation_bn:\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            x = BatchNormalization(center=False, scale=False)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(self.activation_function)(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Residual unit.\"\"\"\n",
    "        x, y = inputs\n",
    "        n_samples_in = y.shape[1]\n",
    "        downsample = n_samples_in // self.n_samples_out\n",
    "        n_filters_in = y.shape[2]\n",
    "        y = self._skip_connection(y, downsample, n_filters_in)\n",
    "        # 1st layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n",
    "                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n",
    "        x = self._batch_norm_plus_activation(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        # 2nd layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n",
    "                   padding='same', use_bias=False,\n",
    "                   kernel_initializer=self.kernel_initializer)(x)\n",
    "        if self.preactivation:\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            y = x\n",
    "            x = self._batch_norm_plus_activation(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "            y = x\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "def get_model(n_classes, last_layer='sigmoid'):\n",
    "    kernel_size = 16\n",
    "    kernel_initializer = 'he_normal'\n",
    "    signal = Input(shape=(4096, 12), dtype=np.float32, name='signal')\n",
    "    x = signal\n",
    "    x = Conv1D(64, kernel_size, padding='same', use_bias=False,\n",
    "               kernel_initializer=kernel_initializer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x, y = ResidualUnit(1024, 128, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, x])\n",
    "    x, y = ResidualUnit(256, 196, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, y = ResidualUnit(64, 256, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, _ = ResidualUnit(16, 320, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x = Flatten()(x)\n",
    "    diagn = Dense(n_classes, activation=last_layer, kernel_initializer=kernel_initializer)(x)\n",
    "    model = Model(signal, diagn)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Parameters**\n",
    "\n",
    "Loading the parameters for the model that were found in the paper. We will call this our first model\n",
    "\n",
    "They trained 10 NN with different initializations. The choose the model to use based on the median micro average persion (mAP = 0.951). They had to choose the one right above the median since 10 is even so they can't take the median execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = cwd + '\\\\model\\\\model.hdf5'\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "opt = Adam(lr)\n",
    "\n",
    "model_1 = load_model(path_to_model, compile=False)\n",
    "model_1.compile(loss=loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "Here is the class for transforming the data into the proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGSequence(Sequence):\n",
    "    @classmethod\n",
    "    def get_train_and_val(cls, tracings: np.ndarray, labels: np.ndarray=None, batch_size=8, val_split=0.02):\n",
    "        n_samples = tracings.shape[0]\n",
    "        n_train = math.ceil(n_samples*(1-val_split))\n",
    "        train_seq = cls(tracings, labels, batch_size, end_idx=n_train)\n",
    "        valid_seq = cls(tracings, labels, batch_size, start_idx=n_train)\n",
    "        return train_seq, valid_seq\n",
    "\n",
    "    def __init__(self, tracings:np.ndarray, labels:np.ndarray=None, batch_size:int=8,\\\n",
    "        start_idx=0, end_idx=None):\n",
    "        if labels is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = labels\n",
    "        # Get tracings\n",
    "        self.x = tracings\n",
    "        self.batch_size = batch_size\n",
    "        if end_idx is None:\n",
    "            end_idx = len(self.x)\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return self.y.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.start_idx + idx * self.batch_size\n",
    "        end = min(start + self.batch_size, self.end_idx)\n",
    "        if self.y is None:\n",
    "            return np.array(self.x[start:end, :, :])\n",
    "        else:\n",
    "            return np.array(self.x[start:end, :, :]), np.array(self.y[start:end])\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Function**\n",
    "\n",
    "We will also train the model with the data accessible for better comparison with the simplified model. We will call this the second model\n",
    "\n",
    "For sake of computational resources and time, the second model was only trained once instead of trained 10 times and then taking the model based on the median mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN:\n",
    "    def __init__(self, loss, opt, verbose, save_path=None):\n",
    "        # Optimization settings\n",
    "        self.callbacks = [ReduceLROnPlateau(monitor='val_loss',\n",
    "                            factor=0.1,\n",
    "                            patience=7,\n",
    "                            min_lr=lr / 100),\n",
    "                            EarlyStopping(monitor='val_loss', \n",
    "                            patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n",
    "                            min_delta=0.00001)]\n",
    "\n",
    "        self.loss = loss\n",
    "        self.optimizer = opt\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Save the BEST and LAST model\n",
    "        if not save_path is None:\n",
    "            self.callbacks += [ModelCheckpoint(save_path, save_best_only=True)]\n",
    "\n",
    "    def train(self, train_seq, val_seq):\n",
    "        self.model = get_model(train_seq.n_classes)\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        # Train neural network\n",
    "        self.model.fit(train_seq,\n",
    "            epochs=50,\n",
    "            initial_epoch=0,  # If you are continuing a interrupted section change here\n",
    "            callbacks=self.callbacks,\n",
    "            validation_data=val_seq,\n",
    "            verbose=self.verbose)\n",
    "\n",
    "    def predict(self, test_seq):\n",
    "        return self.model.predict(test_seq,  verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Models\n",
    "\n",
    "Need to choose what model I want\n",
    "\n",
    "Going to have to use something like random forest because I need a multi-label classifier, or I can use sklearn.multioutput.MultiOutputClassifier and use any classifier\n",
    "\n",
    "I think all of the data for all 12 leads is the set of features for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Tune the hyperparameters\n",
    "class RF_Model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = RandomForestClassifier(verbose=verbose)\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class LR_model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = MultiOutputClassifier(LogisticRegression(verbose=verbose))\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "PCA for the simplified models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_Transform:\n",
    "    def __init__(self, r:int):\n",
    "        self.PCA_instance = PCA(n_components=r)\n",
    "\n",
    "    def _processData(self, X: np.ndarray):\n",
    "        self.preprocess = StandardScaler()\n",
    "        self.preprocess.fit(X)\n",
    "\n",
    "    def FitData(self, X: np.ndarray):\n",
    "        self._processData(X)\n",
    "        self.PCA_instance.fit(self.preprocess.transform(X))\n",
    "\n",
    "    def TransformData(self, X_train: np.ndarray, X_test: np.ndarray):\n",
    "        X_train = self.preprocess.transform(X_train)\n",
    "        X_test = self.preprocess.transform(X_test)\n",
    "        return self.PCA_instance.transform(X_train), self.PCA_instance.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold\n",
    "\n",
    "K-fold procedure for validation of the models\n",
    "\n",
    "They use a validation set of 2% so something to think about\n",
    "\n",
    "They didn't round for the outputs, seems to be a threshold in which they consider it to occur\n",
    "\n",
    "They used precision-recall curves for things, but in total found precision, recall, specificity and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Outputs**\n",
    "\n",
    "Function for processing the output probabilities from the CNNs\n",
    "\n",
    "Need to find the optimal thresholds for each test set before running the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindThresholds(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    '''Optimize the thresholds based on the F1-score for the model 1 in the test set'''\n",
    "\n",
    "    start = 0.05\n",
    "    end = 0.8\n",
    "    step = 0.001\n",
    "    threshold_options = np.linspace(start,end,int((end-start)/step))\n",
    "    optimal_thresholds = np.array([])\n",
    "\n",
    "    for ii in range(y_pred.shape[1]):\n",
    "        #use the np greater for a list of thresholds and then find max F1-score and index it back to the thresholds\n",
    "        F1_scores = [f1_score(y_true=y_true[:,ii], y_pred=ProcessOutputs(y_pred[:,ii], threshold)) for threshold in threshold_options]\n",
    "        optimal_thresholds = np.append(optimal_thresholds, threshold_options[np.argmax(F1_scores)])\n",
    "\n",
    "    return optimal_thresholds\n",
    "\n",
    "\n",
    "def ProcessOutputs(outputs: np.ndarray, thresholds: np.ndarray):\n",
    "\n",
    "    threshold_check = np.array([np.greater_equal(sample, thresholds) for sample in outputs])\n",
    "    threshold_outputs = threshold_check.astype(int)\n",
    "    \n",
    "    return threshold_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Function**\n",
    "\n",
    "Making a function to be able to call all of the metrics each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score_initator():\n",
    "\n",
    "        scores = dict()\n",
    "        metrics = ['Precision', 'Recall', 'Specificity', 'F1']\n",
    "        models = ['m1', 'm2', 'm3', 'm4']\n",
    "        for ii, metric in enumerate(metrics):\n",
    "                scores[metric] = dict()\n",
    "                for jj, model in enumerate(models):\n",
    "                        scores[metric][model] = dict()\n",
    "                        for zz, abnomality in enumerate(abnormalities):\n",
    "                                scores[metric][model][abnomality] = np.array([])\n",
    "        \n",
    "        return scores\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    m = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    spc = m[0, 0] * 1.0 / (m[0, 0] + m[0, 1])\n",
    "    return spc\n",
    "\n",
    "def Find_metrics(scores: dict, model_name: str, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \n",
    "        def _findProperScore(metric, y_true, y_pred):\n",
    "                if metric == 'Precision':\n",
    "                        return precision_score(y_true, y_pred)  \n",
    "                elif metric == 'Recall':\n",
    "                        return recall_score(y_true, y_pred)\n",
    "                elif metric == 'Specificity':\n",
    "                        return specificity_score(y_true, y_pred)\n",
    "                else:\n",
    "                        return f1_score(y_true, y_pred)\n",
    "\n",
    "        for jj, metric in enumerate(scores.keys()):\n",
    "                for ii, cardio_class in enumerate(scores[metric][model_name].keys()):\n",
    "\n",
    "                        scores[metric][model_name][cardio_class] = np.append(scores[metric][model_name][cardio_class], \\\n",
    "                                _findProperScore(metric, y_true[:,ii], y_pred[:,ii]))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "train_scores = Score_initator()\n",
    "\n",
    "#Initilaize the models that need to be trained\n",
    "model_verbose = 1\n",
    "model_2 = MyCNN(loss, opt, verbose = model_verbose)\n",
    "'''model_3 = RF_Model(verbose = model_verbose)\n",
    "model_4 = LR_model(verbose = model_verbose)'''\n",
    "\n",
    "#PCA initlization\n",
    "#PCA_transformer = PCA_Transform(r = 10)\n",
    "\n",
    "#TODO: there is no regularization in the layers so maybe can add that\n",
    "for train_index, test_index in kf.split(X = tracings_train[:,1,1], y = labels_train):\n",
    "\n",
    "        X_train, X_test = tracings_train[train_index,:,:], tracings_train[test_index,:,:]\n",
    "        y_train, y_test = labels_train[train_index], labels_train[test_index]\n",
    "\n",
    "        #Put data in sequence for models 1 and 2 (CNN)\n",
    "        train_seq, val_seq = ECGSequence.get_train_and_val(X_train, y_train, batch_size=64)\n",
    "        test_seq = ECGSequence(X_test, y_test, batch_size=64)\n",
    "\n",
    "        #Transform data with PCA for models 3 and 4\n",
    "        '''for ii in range(X_train.shape[-1]):\n",
    "                PCA_transformer.FitData(X_train[:,:,ii])\n",
    "                PCA_X_train_temp, PCA_X_test_temp = PCA_transformer.\\\n",
    "                        TransformData(X_train[:,:,ii], X_test[:,:,ii])\n",
    "                if ii == 0:\n",
    "                        PCA_X_train = PCA_X_train_temp\n",
    "                        PCA_X_test = PCA_X_test_temp\n",
    "                else:\n",
    "                        PCA_X_train = np.append(PCA_X_train, PCA_X_train_temp, axis = 1)\n",
    "                        PCA_X_test = np.append(PCA_X_test, PCA_X_test_temp, axis = 1)'''\n",
    "\n",
    "        #Train models\n",
    "        print('\\n-------------------Training model 2----------------------')\n",
    "        model_2.train(train_seq, val_seq=val_seq)\n",
    "        '''print('\\n-------------------Training model 3----------------------')\n",
    "        model_3.train(X = PCA_X_train, y = y_train)\n",
    "        print('\\n-------------------Training model 4----------------------')\n",
    "        model_4.train(X = PCA_X_train, y = y_train)'''\n",
    "\n",
    "        #Predict with the models - need to find optimal thresholds first for this test set\n",
    "        print('\\n-------------------Testing model 1----------------------')\n",
    "        optimal_thresholds = FindThresholds(model_1.predict(test_seq, verbose=1), y_test)\n",
    "        m1_pred = ProcessOutputs(model_1.predict(test_seq, verbose=1), optimal_thresholds)\n",
    "        print('\\n-------------------Testing model 2----------------------')\n",
    "        m2_pred = ProcessOutputs(model_2.predict(test_seq), optimal_thresholds)\n",
    "        print('\\n-------------------Testing model 3----------------------')\n",
    "        #m3_pred = model_3.predict(PCA_X_train)\n",
    "        print('\\n-------------------Testing model 4----------------------')\n",
    "        #m4_pred = model_4.predict(PCA_X_train)\n",
    "\n",
    "        #Find scores\n",
    "        print('\\n-------------------Finding scores----------------------')\n",
    "        train_scores = Find_metrics(train_scores, 'm1', y_test, m1_pred)\n",
    "        train_scores = Find_metrics(train_scores, 'm2', y_test, m2_pred)\n",
    "        '''train_scores = Find_metrics(train_scores, 'm3', y_test, m3_pred)\n",
    "        train_scores = Find_metrics(train_scores, 'm4', y_test, m4_pred)'''\n",
    "\n",
    "\n",
    "save_path = cwd + '\\\\MyOutputs\\\\TrainScores.pickle'\n",
    "SaveObject(train_scores, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing (ADD PCA Calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Models**\n",
    "\n",
    "Train all the models that need to be trained on the entire training set and then save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_1 = cwd + '\\\\MyOutputs\\\\Model1.pickle'\n",
    "save_path_2 = cwd + '\\\\MyOutputs\\\\Model2.pickle'\n",
    "save_path_3 = cwd + '\\\\MyOutputs\\\\Model3.pickle'\n",
    "save_path_4 = cwd + '\\\\MyOutputs\\\\Model4.pickle'\n",
    "\n",
    "train_seq, val_seq = ECGSequence.get_train_and_val(tracings_train, labels_train, batch_size=64)\n",
    "#Train models\n",
    "#Here the validation seq is just the test seq since we are using a k-fold analysis\n",
    "print('\\n-------------------Training model 2----------------------')\n",
    "try:\n",
    "    model_2 = LoadObject(save_path_2)\n",
    "except:\n",
    "    #Have to load a new model since we want to save the best one here\n",
    "    model_2 = MyCNN(loss, opt, verbose = model_verbose, save_path=cwd+'\\\\MyOutputs\\\\Model2.hdf5')\n",
    "    model_2.train(train_seq, val_seq=val_seq)\n",
    "'''print('\\n-------------------Training model 3----------------------')\n",
    "try:\n",
    "    model_3 = LoadObject(save_path_3)\n",
    "except:\n",
    "    model_3.train(X = PCA_X_train, y = y_train)\n",
    "print('\\n-------------------Training model 4----------------------')\n",
    "try:\n",
    "    model_4 = LoadObject(save_path_4)\n",
    "except:\n",
    "    model_4.train(X = PCA_X_train, y = y_train)'''\n",
    "\n",
    "#Save the models\n",
    "SaveObject(model_1, save_path_1)\n",
    "SaveObject(model_2, save_path_2)\n",
    "'''SaveObject(model_3, save_path_3)\n",
    "SaveObject(model_4, save_path_4)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Models**\n",
    "\n",
    "Test all the models with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = Score_initator()\n",
    "test_seq = ECGSequence(tracings_train, labels_train, batch_size=64)\n",
    "\n",
    "#Predict with the models\n",
    "print('\\n-------------------Testing model 1----------------------')\n",
    "optimal_thresholds = FindThresholds(model_1.predict(test_seq, verbose=1), labels_train)\n",
    "m1_pred = ProcessOutputs(model_1.predict(test_seq, verbose=1), optimal_thresholds)\n",
    "print('\\n-------------------Testing model 2----------------------')\n",
    "m2_pred = ProcessOutputs(model_2.predict(test_seq), optimal_thresholds)\n",
    "print('\\n-------------------Testing model 3----------------------')\n",
    "#m3_pred = model_3.predict(PCA_X_test)\n",
    "print('\\n-------------------Testing model 4----------------------')\n",
    "#m4_pred = model_4.predict(PCA_X_test)\n",
    "\n",
    "\n",
    "\n",
    "test_scores = Find_metrics(test_scores, 'm1', labels_train, m1_pred)\n",
    "#test_scores = Find_metrics(test_scores, 'm2', labels_test, m2_pred)\n",
    "save_path = cwd + '\\\\MyOutputs\\\\TestScores.pickle'\n",
    "SaveObject(test_scores, save_path)\n",
    "print(test_scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
