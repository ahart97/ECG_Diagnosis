{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ECG Diagnosis Code**\n",
    "\n",
    "This code is based on the code developed here: https://doi.org/10.1038/s41467-020-15432-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n",
    "                                        CSVLogger, EarlyStopping)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Load in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in test data\n",
    "path_to_hdf5 = cwd + '\\\\data\\\\test\\\\ecg_tracings.hdf5'\n",
    "dataset_name = 'tracings'\n",
    "path_to_csv = cwd + '\\\\data\\\\test\\\\gold_standard.csv'\n",
    "\n",
    "#Order is based on test set csv\n",
    "abnormalities = ['1dAVb','RBBB', 'LBBB', 'SB',  'AF', 'ST']\n",
    "\n",
    "labels_test = pd.read_csv(path_to_csv).values\n",
    "f = h5py.File(path_to_hdf5, \"r\")\n",
    "tracings_test = f[dataset_name][()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation of the class probabilities in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPercents(labels:np.ndarray, average: bool=True):\n",
    "    bias = []\n",
    "    for ii in range(labels.shape[-1]):\n",
    "        bias.append(np.sum(labels[:,ii])/len(labels[:,ii]))\n",
    "    \n",
    "    if average:\n",
    "        bias = np.mean(bias)\n",
    "        return bias\n",
    "    else:\n",
    "        return np.array(bias)\n",
    "\n",
    "test_per = FindPercents(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Data Functions**\n",
    "\n",
    "A function for saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveObject(object_to_save, save_path):\n",
    "    '''Load path must have the file name with the .pickle extension'''\n",
    "    pickle_out = open(save_path, 'wb')\n",
    "    pickle.dump(object_to_save, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadObject(load_path):\n",
    "    infile = open(load_path, 'rb')\n",
    "    Loaded_object = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    return Loaded_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the NN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(object):\n",
    "    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n",
    "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "                 postactivation_bn=False, activation_function='relu'):\n",
    "        self.n_samples_out = n_samples_out\n",
    "        self.n_filters_out = n_filters_out\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.dropout_rate = 1 - dropout_keep_prob\n",
    "        self.kernel_size = kernel_size\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _skip_connection(self, y, downsample, n_filters_in):\n",
    "        \"\"\"Implement skip connection.\"\"\"\n",
    "        # Deal with downsampling\n",
    "        if downsample > 1:\n",
    "            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n",
    "        elif downsample == 1:\n",
    "            y = y\n",
    "        else:\n",
    "            raise ValueError(\"Number of samples should always decrease.\")\n",
    "        # Deal with n_filters dimension increase\n",
    "        if n_filters_in != self.n_filters_out:\n",
    "            # This is one of the two alternatives presented in ResNet paper\n",
    "            # Other option is to just fill the matrix with zeros.\n",
    "            y = Conv1D(self.n_filters_out, 1, padding='same',\n",
    "                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n",
    "        return y\n",
    "\n",
    "    def _batch_norm_plus_activation(self, x):\n",
    "        if self.postactivation_bn:\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            x = BatchNormalization(center=False, scale=False)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(self.activation_function)(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Residual unit.\"\"\"\n",
    "        x, y = inputs\n",
    "        n_samples_in = y.shape[1]\n",
    "        downsample = n_samples_in // self.n_samples_out\n",
    "        n_filters_in = y.shape[2]\n",
    "        y = self._skip_connection(y, downsample, n_filters_in)\n",
    "        # 1st layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n",
    "                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n",
    "        x = self._batch_norm_plus_activation(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        # 2nd layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n",
    "                   padding='same', use_bias=False,\n",
    "                   kernel_initializer=self.kernel_initializer)(x)\n",
    "        if self.preactivation:\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            y = x\n",
    "            x = self._batch_norm_plus_activation(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "            y = x\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "def get_model(n_classes, last_layer='sigmoid'):\n",
    "    kernel_size = 16\n",
    "    kernel_initializer = 'he_normal'\n",
    "    signal = Input(shape=(4096, 12), dtype=np.float32, name='signal')\n",
    "    x = signal\n",
    "    x = Conv1D(64, kernel_size, padding='same', use_bias=False,\n",
    "               kernel_initializer=kernel_initializer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x, y = ResidualUnit(1024, 128, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, x])\n",
    "    x, y = ResidualUnit(256, 196, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, y = ResidualUnit(64, 256, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, _ = ResidualUnit(16, 320, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x = Flatten()(x)\n",
    "    diagn = Dense(n_classes, activation=last_layer, kernel_initializer=kernel_initializer)(x)\n",
    "    model = Model(signal, diagn)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Parameters**\n",
    "\n",
    "Loading the parameters for the model that were found in the paper. We will call this our first model\n",
    "\n",
    "They trained 10 NN with different initializations. The choose the model to use based on the median micro average persion (mAP = 0.951). They had to choose the one right above the median since 10 is even so they can't take the median execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = cwd + '\\\\model\\\\model.hdf5'\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "opt = Adam(lr)\n",
    "\n",
    "model_1 = load_model(path_to_model, compile=False)\n",
    "model_1.compile(loss=loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "Here is the class for transforming the data into the proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGSequence(Sequence):\n",
    "    @classmethod\n",
    "    def get_train_and_val(cls, tracings: np.ndarray, labels: np.ndarray=None, batch_size=8, val_split=0.02):\n",
    "        n_samples = tracings.shape[0]\n",
    "        n_train = math.ceil(n_samples*(1-val_split))\n",
    "        train_seq = cls(tracings, labels, batch_size, end_idx=n_train)\n",
    "        valid_seq = cls(tracings, labels, batch_size, start_idx=n_train)\n",
    "        return train_seq, valid_seq\n",
    "\n",
    "    def __init__(self, tracings:np.ndarray, labels:np.ndarray=None, batch_size:int=8,\\\n",
    "        start_idx=0, end_idx=None):\n",
    "        if labels is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = labels\n",
    "        # Get tracings\n",
    "        self.x = tracings\n",
    "        self.batch_size = batch_size\n",
    "        if end_idx is None:\n",
    "            end_idx = len(self.x)\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return self.y.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.start_idx + idx * self.batch_size\n",
    "        end = min(start + self.batch_size, self.end_idx)\n",
    "        if self.y is None:\n",
    "            return np.array(self.x[start:end, :, :])\n",
    "        else:\n",
    "            return np.array(self.x[start:end, :, :]), np.array(self.y[start:end])\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Function**\n",
    "\n",
    "We will also train the model with the data accessible for better comparison with the simplified model. We will call this the second model\n",
    "\n",
    "For sake of computational resources and time, the second model was only trained once instead of trained 10 times and then taking the model based on the median mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN:\n",
    "    def __init__(self, loss, opt, verbose, save_path=None):\n",
    "        # Optimization settings\n",
    "        self.callbacks = [ReduceLROnPlateau(monitor='val_loss',\n",
    "                            factor=0.1,\n",
    "                            patience=7,\n",
    "                            min_lr=lr / 100),\n",
    "                            EarlyStopping(monitor='val_loss', \n",
    "                            patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n",
    "                            min_delta=0.00001)]\n",
    "\n",
    "        self.loss = loss\n",
    "        self.optimizer = opt\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Save the BEST and LAST model\n",
    "        if not save_path is None:\n",
    "            self.callbacks += [ModelCheckpoint(save_path, save_best_only=True)]\n",
    "\n",
    "    def train(self, train_seq, val_seq):\n",
    "        self.model = get_model(train_seq.n_classes)\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        # Train neural network\n",
    "        self.model.fit(train_seq,\n",
    "            epochs=50,\n",
    "            initial_epoch=0,  # If you are continuing a interrupted section change here\n",
    "            callbacks=self.callbacks,\n",
    "            validation_data=val_seq,\n",
    "            verbose=self.verbose)\n",
    "\n",
    "    def predict(self, test_seq):\n",
    "        return self.model.predict(test_seq,  verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Models\n",
    "\n",
    "Need to choose what model I want\n",
    "\n",
    "Going to have to use something like random forest because I need a multi-label classifier, or I can use sklearn.multioutput.MultiOutputClassifier and use any classifier\n",
    "\n",
    "I think all of the data for all 12 leads is the set of features for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Tune the hyperparameters\n",
    "class RF_Model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = RandomForestClassifier(verbose=verbose)\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class LR_model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = MultiOutputClassifier(LogisticRegression(verbose=verbose))\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "PCA for the simplified models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_Transform:\n",
    "    def __init__(self, r:int):\n",
    "        self.PCA_instance = PCA(n_components=r)\n",
    "\n",
    "    def _processData(self, X: np.ndarray):\n",
    "        self.preprocess = StandardScaler()\n",
    "        self.preprocess.fit(X)\n",
    "\n",
    "    def FitData(self, X: np.ndarray):\n",
    "        self._processData(X)\n",
    "        self.PCA_instance.fit(self.preprocess.transform(X))\n",
    "\n",
    "    def TransformData(self, X_train: np.ndarray, X_test: np.ndarray):\n",
    "        X_train = self.preprocess.transform(X_train)\n",
    "        X_test = self.preprocess.transform(X_test)\n",
    "        return self.PCA_instance.transform(X_train), self.PCA_instance.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold (FIX paragraph below)\n",
    "\n",
    "K-fold procedure for validation of the models\n",
    "\n",
    "They use a validation set of 2% so something to think about\n",
    "\n",
    "They didn't round for the outputs, seems to be a threshold in which they consider it to occur\n",
    "\n",
    "They used precision-recall curves for things, but in total found precision, recall, specificity and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Outputs**\n",
    "\n",
    "Function for processing the output probabilities from the CNNs\n",
    "\n",
    "Need to find the optimal thresholds to maximize F1 score for the test set before running the processing\n",
    "\n",
    "Will use these thresholds for the other CNN as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindThresholds(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    '''Optimize the thresholds based on the F1-score for the model 1 in the test set'''\n",
    "\n",
    "    start = 0.05\n",
    "    end = 0.8\n",
    "    step = 0.001\n",
    "    threshold_options = np.linspace(start,end,int((end-start)/step))\n",
    "    optimal_thresholds = np.array([])\n",
    "\n",
    "    for ii in range(y_pred.shape[1]):\n",
    "        #use the np greater for a list of thresholds and then find max F1-score and index it back to the thresholds\n",
    "        F1_scores = [f1_score(y_true=y_true[:,ii], y_pred=ProcessOutputs(y_pred[:,ii], threshold), zero_division=0) for threshold in threshold_options]\n",
    "        optimal_thresholds = np.append(optimal_thresholds, threshold_options[np.argmax(F1_scores)])\n",
    "\n",
    "    return optimal_thresholds\n",
    "\n",
    "\n",
    "def ProcessOutputs(outputs: np.ndarray, thresholds: np.ndarray):\n",
    "\n",
    "    threshold_check = np.array([np.greater_equal(sample, thresholds) for sample in outputs])\n",
    "    threshold_outputs = threshold_check.astype(int)\n",
    "    \n",
    "    return threshold_outputs\n",
    "\n",
    "\n",
    "#Find optimal thresholds with test set\n",
    "test_seq = ECGSequence(tracings_test, labels_test, batch_size=32)\n",
    "optimal_thresholds = FindThresholds(model_1.predict(test_seq), labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Function**\n",
    "\n",
    "Making a function to be able to call all of the metrics each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score_initator(filler):\n",
    "\n",
    "        scores = dict()\n",
    "        metrics = ['Accuracy','Precision', 'Recall', 'Specificity', 'F1']\n",
    "        for ii, metric in enumerate(metrics):\n",
    "                scores[metric] = dict()\n",
    "                for zz, abnomality in enumerate(abnormalities):\n",
    "                        scores[metric][abnomality] = filler\n",
    "        \n",
    "        return scores\n",
    "\n",
    "def Find_metrics(scores: dict, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \n",
    "        def _findProperScore(metric, y_true, y_pred):\n",
    "\n",
    "                m = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "                if metric == 'Accuracy':\n",
    "                        return accuracy_score(y_true, y_pred)\n",
    "                elif metric == 'Precision':\n",
    "                        return precision_score(y_true, y_pred, zero_division=0)  \n",
    "                elif metric == 'Recall':\n",
    "                        return recall_score(y_true, y_pred, zero_division=0)\n",
    "                elif metric == 'Specificity':\n",
    "                        return m[0, 0] * 1.0 / (m[0, 0] + m[0, 1])\n",
    "                else:\n",
    "                        return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        for jj, metric in enumerate(scores.keys()):\n",
    "                for ii, cardio_class in enumerate(scores[metric].keys()):\n",
    "\n",
    "                        scores[metric][cardio_class] = np.append(scores[metric][cardio_class], \\\n",
    "                                _findProperScore(metric, y_true[:,ii], y_pred[:,ii]))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold #2\n",
      "Fold #3\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state = 42)\n",
    "\n",
    "m1_scores = Score_initator(filler = np.array([]))\n",
    "m2_scores = Score_initator(filler = np.array([]))\n",
    "\n",
    "#Initilaize the models that need to be trained\n",
    "model_verbose = 0\n",
    "model_2 = MyCNN(loss, opt, verbose = model_verbose)\n",
    "'''model_3 = RF_Model(verbose = model_verbose)'''\n",
    "\n",
    "#PCA initlization\n",
    "#PCA_transformer = PCA_Transform(r = 10)\n",
    "\n",
    "fold_count =1 \n",
    "#TODO: there is no regularization in the layers so maybe can add that\n",
    "for train_index, test_index in kf.split(X = tracings_test[:,1,1]):\n",
    "\n",
    "        print('Fold #{}'.format(fold_count))\n",
    "\n",
    "        X_train, X_test = tracings_test[train_index,:,:], tracings_test[test_index,:,:]\n",
    "        y_train, y_test = labels_test[train_index], labels_test[test_index]\n",
    "\n",
    "        #Put data in sequence for models 1 and 2 (CNN)\n",
    "        train_seq, val_seq = ECGSequence.get_train_and_val(X_train, y_train, batch_size=64)\n",
    "        test_seq = ECGSequence(X_test, y_test, batch_size=32)\n",
    "\n",
    "        #Transform data with PCA for models 3 and 4\n",
    "        '''for ii in range(X_train.shape[-1]):\n",
    "                PCA_transformer.FitData(X_train[:,:,ii])\n",
    "                PCA_X_train_temp, PCA_X_test_temp = PCA_transformer.\\\n",
    "                        TransformData(X_train[:,:,ii], X_test[:,:,ii])\n",
    "                if ii == 0:\n",
    "                        PCA_X_train = PCA_X_train_temp\n",
    "                        PCA_X_test = PCA_X_test_temp\n",
    "                else:\n",
    "                        PCA_X_train = np.append(PCA_X_train, PCA_X_train_temp, axis = 1)\n",
    "                        PCA_X_test = np.append(PCA_X_test, PCA_X_test_temp, axis = 1)'''\n",
    "\n",
    "        #Train models\n",
    "        model_2.train(train_seq, val_seq=val_seq)\n",
    "        '''model_3.train(X = PCA_X_train, y = y_train)'''\n",
    "\n",
    "        #Predict with the models - need to find optimal thresholds first for this test set\n",
    "        m1_pred = ProcessOutputs(model_1.predict(test_seq, verbose=model_verbose), optimal_thresholds)\n",
    "        m2_pred = ProcessOutputs(model_2.predict(test_seq), optimal_thresholds)\n",
    "        #m3_pred = model_3.predict(PCA_X_train)\n",
    "\n",
    "        #Find scores\n",
    "        m1_scores = Find_metrics(m1_scores, y_test, m1_pred)\n",
    "        m2_scores = Find_metrics(m2_scores, y_test, m2_pred)\n",
    "        '''train_scores = Find_metrics(train_scores, 'm3', y_test, m3_pred)'''\n",
    "\n",
    "        fold_count+=1\n",
    "\n",
    "\n",
    "m1_scores_path = cwd + '\\\\MyOutputs\\\\m1_scores.pickle'\n",
    "SaveObject(m1_scores, m1_scores_path)\n",
    "m2_scores_path = cwd + '\\\\MyOutputs\\\\m2_scores.pickle'\n",
    "SaveObject(m2_scores, m2_scores_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hart\\AppData\\Local\\Temp\\ipykernel_14920\\2549717226.py:7: RuntimeWarning: Mean of empty slice\n",
      "  avg_scores[sub_metric][cardiac_class] = {'Average': np.nanmean(sub_score),\\\n",
      "C:\\Users\\Andrew Hart\\AppData\\Local\\Temp\\ipykernel_14920\\2549717226.py:8: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  'Var': np.nanvar(sub_score)}\n"
     ]
    }
   ],
   "source": [
    "def ProcessScores(scores: dict, save_path: str):\n",
    "    avg_scores = Score_initator(filler=dict())\n",
    "    for sub_metric in avg_scores.keys():\n",
    "        for cardiac_class in avg_scores[sub_metric].keys():\n",
    "            sub_score = scores[sub_metric][cardiac_class]\n",
    "            sub_score[sub_score == 0.0] = np.NaN\n",
    "            avg_scores[sub_metric][cardiac_class] = {'Average': np.nanmean(sub_score),\\\n",
    "                'Var': np.nanvar(sub_score)}\n",
    "\n",
    "    score_output = pd.DataFrame.from_dict({(i,j): avg_scores[i][j]\n",
    "                           for i in avg_scores.keys() \n",
    "                           for j in avg_scores[i].keys()},\n",
    "                       orient='index')\n",
    "\n",
    "    score_output.to_csv(save_path)\n",
    "\n",
    "\n",
    "m1_scores = LoadObject(m1_scores_path)\n",
    "m2_scores = LoadObject(m2_scores_path)\n",
    "\n",
    "ProcessScores(m1_scores, cwd + '\\\\MyOutputs\\\\m1_scores.csv')\n",
    "ProcessScores(m2_scores, cwd + '\\\\MyOutputs\\\\m2_scores.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
