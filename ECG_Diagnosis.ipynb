{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ECG Diagnosis Code**\n",
    "\n",
    "This code is based on the code developed here: https://doi.org/10.1038/s41467-020-15432-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n",
    "                                        CSVLogger, EarlyStopping)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessLabels(data_info: pd.DataFrame, train_ids: np.ndarray, abnormalities: list):\n",
    "    good_rows = []\n",
    "    bad_rows = []\n",
    "    labels = np.array([])\n",
    "    for ii, id in enumerate(train_ids):\n",
    "        if id in data_info['exam_id'].to_numpy():\n",
    "            good_rows.append(ii)\n",
    "            data_row = data_info.iloc[np.where(data_info['exam_id'].to_numpy() == id)[0][0]]\n",
    "            labels = np.append(labels, data_row[abnormalities].values.tolist())\n",
    "        else:\n",
    "            bad_rows.append(ii)\n",
    "\n",
    "    labels = np.reshape(labels, (len(good_rows), len(abnormalities)))\n",
    "    \n",
    "    return labels.astype('int64'), good_rows\n",
    "\n",
    "#Order is based on test set csv\n",
    "abnormalities = ['1dAVb','RBBB', 'LBBB', 'SB',  'AF', 'ST']\n",
    "\n",
    "#Load in training data\n",
    "path_to_hdf5 = cwd + '\\\\data\\\\train\\\\exams_part17.hdf5'\n",
    "dataset_name = 'tracings'\n",
    "path_to_csv = cwd + '\\\\data\\\\train\\\\exams.csv'\n",
    "\n",
    "#Process the exams data\n",
    "f = h5py.File(path_to_hdf5, \"r\")\n",
    "tracings_train = f[dataset_name][()]\n",
    "train_ids = f['exam_id'][()]\n",
    "f.close()\n",
    "labels_train, good_rows = ProcessLabels(pd.read_csv(path_to_csv), train_ids, abnormalities)\n",
    "tracings_train = tracings_train[good_rows]\n",
    "\n",
    "#Load in test data\n",
    "path_to_hdf5 = cwd + '\\\\data\\\\test\\\\ecg_tracings.hdf5'\n",
    "dataset_name = 'tracings'\n",
    "path_to_csv = cwd + '\\\\data\\\\test\\\\gold_standard.csv'\n",
    "\n",
    "labels_test = pd.read_csv(path_to_csv).values\n",
    "f = h5py.File(path_to_hdf5, \"r\")\n",
    "tracings_test = f[dataset_name][()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the NN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(object):\n",
    "    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n",
    "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
    "                 postactivation_bn=False, activation_function='relu'):\n",
    "        self.n_samples_out = n_samples_out\n",
    "        self.n_filters_out = n_filters_out\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.dropout_rate = 1 - dropout_keep_prob\n",
    "        self.kernel_size = kernel_size\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _skip_connection(self, y, downsample, n_filters_in):\n",
    "        \"\"\"Implement skip connection.\"\"\"\n",
    "        # Deal with downsampling\n",
    "        if downsample > 1:\n",
    "            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n",
    "        elif downsample == 1:\n",
    "            y = y\n",
    "        else:\n",
    "            raise ValueError(\"Number of samples should always decrease.\")\n",
    "        # Deal with n_filters dimension increase\n",
    "        if n_filters_in != self.n_filters_out:\n",
    "            # This is one of the two alternatives presented in ResNet paper\n",
    "            # Other option is to just fill the matrix with zeros.\n",
    "            y = Conv1D(self.n_filters_out, 1, padding='same',\n",
    "                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n",
    "        return y\n",
    "\n",
    "    def _batch_norm_plus_activation(self, x):\n",
    "        if self.postactivation_bn:\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            x = BatchNormalization(center=False, scale=False)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(self.activation_function)(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Residual unit.\"\"\"\n",
    "        x, y = inputs\n",
    "        n_samples_in = y.shape[1]\n",
    "        downsample = n_samples_in // self.n_samples_out\n",
    "        n_filters_in = y.shape[2]\n",
    "        y = self._skip_connection(y, downsample, n_filters_in)\n",
    "        # 1st layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n",
    "                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n",
    "        x = self._batch_norm_plus_activation(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        # 2nd layer\n",
    "        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n",
    "                   padding='same', use_bias=False,\n",
    "                   kernel_initializer=self.kernel_initializer)(x)\n",
    "        if self.preactivation:\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            y = x\n",
    "            x = self._batch_norm_plus_activation(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "        else:\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Add()([x, y])  # Sum skip connection and main connection\n",
    "            x = Activation(self.activation_function)(x)\n",
    "            if self.dropout_rate > 0:\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "            y = x\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "def get_model(n_classes, last_layer='sigmoid'):\n",
    "    kernel_size = 16\n",
    "    kernel_initializer = 'he_normal'\n",
    "    signal = Input(shape=(4096, 12), dtype=np.float32, name='signal')\n",
    "    x = signal\n",
    "    x = Conv1D(64, kernel_size, padding='same', use_bias=False,\n",
    "               kernel_initializer=kernel_initializer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x, y = ResidualUnit(1024, 128, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, x])\n",
    "    x, y = ResidualUnit(256, 196, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, y = ResidualUnit(64, 256, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x, _ = ResidualUnit(16, 320, kernel_size=kernel_size,\n",
    "                        kernel_initializer=kernel_initializer)([x, y])\n",
    "    x = Flatten()(x)\n",
    "    diagn = Dense(n_classes, activation=last_layer, kernel_initializer=kernel_initializer)(x)\n",
    "    model = Model(signal, diagn)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Parameters**\n",
    "\n",
    "Loading the parameters for the model that were found in the paper. We will call this our first model\n",
    "\n",
    "They trained 10 NN with different initializations. The choose the model to use based on the median micro average persion (mAP = 0.951). They had to choose the one right above the median since 10 is even so they can't take the median execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = cwd + '\\\\model\\\\model.hdf5'\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "opt = Adam(lr)\n",
    "\n",
    "model_1 = load_model(path_to_model, compile=False)\n",
    "model_1.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "Here is the class for transforming the data into the proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGSequence(Sequence):\n",
    "    @classmethod\n",
    "    def get_seq(cls, tracings: np.ndarray, labels: np.ndarray=None, batch_size=8):\n",
    "        train_seq = cls(tracings, labels, batch_size)\n",
    "        return train_seq\n",
    "\n",
    "    def __init__(self, tracings:np.ndarray, labels:np.ndarray=None, batch_size:int=8):\n",
    "        if labels is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = labels\n",
    "        # Get tracings\n",
    "        self.x = tracings\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_count = tracings.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return self.y.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        if self.y is None:\n",
    "            return np.array(self.x[start:end, :, :])\n",
    "        else:\n",
    "            return np.array(self.x[start:end, :, :]), np.array(self.y[start:end])\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.sample_count / self.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Function**\n",
    "\n",
    "We will also train the model with the data accessible for better comparison with the simplified model. We will call this the second model\n",
    "\n",
    "For sake of computational resources and time, the second model was only trained once instead of trained 10 times and then taking the model based on the median mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN:\n",
    "    def __init__(self, loss, opt, verbose):\n",
    "        # Optimization settings\n",
    "        self.callbacks = [ReduceLROnPlateau(monitor='val_loss',\n",
    "                            factor=0.1,\n",
    "                            patience=7,\n",
    "                            min_lr=lr / 100),\n",
    "                            EarlyStopping(monitor='val_loss', \n",
    "                            patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n",
    "                            min_delta=0.00001)]\n",
    "\n",
    "        self.loss = loss\n",
    "        self.optimizer = opt\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Save the BEST and LAST model\n",
    "        '''callbacks += [ModelCheckpoint('./backup_model_last.hdf5'),\n",
    "        ModelCheckpoint('./backup_model_best.hdf5', save_best_only=True)]'''\n",
    "\n",
    "    def train(self, train_seq, val_seq):\n",
    "        self.model = get_model(train_seq.n_classes)\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        # Train neural network\n",
    "        self.model.fit(train_seq,\n",
    "            epochs=70,\n",
    "            initial_epoch=0,  # If you are continuing a interrupted section change here\n",
    "            callbacks=self.callbacks,\n",
    "            validation_data=val_seq,\n",
    "            verbose=self.verbose)\n",
    "\n",
    "    def predict(self, test_seq):\n",
    "        return self.model.predict(test_seq,  verbose=self.verbose)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Models\n",
    "\n",
    "Need to choose what model I want\n",
    "\n",
    "Going to have to use something like random forest because I need a multi-label classifier, or I can use sklearn.multioutput.MultiOutputClassifier and use any classifier\n",
    "\n",
    "I think all of the data for all 12 leads is the set of features for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Tune the hyperparameters\n",
    "class RF_Model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = RandomForestClassifier(verbose=verbose)\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class LR_model:\n",
    "    def __init__(self, verbose = 1):\n",
    "        self.model = MultiOutputClassifier(LogisticRegression(verbose=verbose))\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fromatting**\n",
    "\n",
    "PCA for the simplified models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_Transform:\n",
    "    def __init__(self, r:int):\n",
    "        self.PCA_instance = PCA(n_components=r)\n",
    "    \n",
    "    def _flattenData(self, X:np.ndarray):\n",
    "        return np.hstack(X)\n",
    "\n",
    "    def _processData(self, X: np.ndarray):\n",
    "        X_flat = self._flattenData(X)\n",
    "        self.preprocess = StandardScaler()\n",
    "        self.preprocess.fit(X_flat)\n",
    "\n",
    "    def FitData(self, X: np.ndarray):\n",
    "        self._processData(X)\n",
    "        self.PCA_instance.fit(self.preprocess.transform(self._flattenData(X)))\n",
    "\n",
    "    def TransformData(self, X_train: np.ndarray, X_test: np.ndarray):\n",
    "        X_train = self.preprocess.transform(self._flattenData(X_train))\n",
    "        X_test = self.preprocess.transform(self._flattenData(X_test))\n",
    "        return self.PCA_instance.transform(X_train), self.PCA_instance.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold\n",
    "\n",
    "K-fold procedure for validation of the models\n",
    "\n",
    "They use a validation set of 2% so something to think about\n",
    "\n",
    "They didn't round for the outputs, seems to be a threshold in which they consider it to occur\n",
    "\n",
    "They used precision-recall curves for things, but in total found precision, recall, specificity and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Outputs**\n",
    "\n",
    "Function for processing the output probabilities from the CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Need to optimize these\n",
    "\n",
    "#Thinking I could optimize them with the model 1 for each test set\n",
    "# could do this with minimizing the sum of the absolute difference between the pred and true\n",
    "\n",
    "def FindThresholds(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    '''Optimize the thresholds based on the F1-score for the model 1 in the test set'''\n",
    "\n",
    "    start = 0.05\n",
    "    end = 0.8\n",
    "    step = 0.001\n",
    "    threshold_options = np.linspace(start,end,int((end-start)/step))\n",
    "    optimal_thresholds = np.array([])\n",
    "\n",
    "    for ii in range(y_pred.shape[1]):\n",
    "        #use the np greater for a list of thresholds and then find max F1-score and index it back to the thresholds\n",
    "        F1_scores = [f1_score(y_true=y_true[:,ii], y_pred=ProcessOutputs(y_pred[:,ii], threshold)) for threshold in threshold_options]\n",
    "        optimal_thresholds = np.append(optimal_thresholds, threshold_options[np.argmax(F1_scores)])\n",
    "\n",
    "    return optimal_thresholds\n",
    "\n",
    "\n",
    "def ProcessOutputs(outputs: np.ndarray, thresholds: np.ndarray):\n",
    "\n",
    "    threshold_check = np.array([np.greater_equal(sample, thresholds) for sample in outputs])\n",
    "    threshold_outputs = threshold_check.astype(int)\n",
    "    \n",
    "    return threshold_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Function**\n",
    "\n",
    "Making a function to be able to call all of the metrics each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''scores = {'Precision': {'Model_1': np.array([]), 'Model_2': np.array([]), 'Model_3': np.array([]), 'Model_4': np.array([])},\\\n",
    "        'Recall': {'Model_1': np.array([]), 'Model_2': np.array([]), 'Model_3': np.array([]), 'Model_4': np.array([])},\\\n",
    "        'F1': {'Model_1': np.array([]), 'Model_2': np.array([]), 'Model_3': np.array([]), 'Model_4': np.array([])}}'''\n",
    "\n",
    "def Score_initator():\n",
    "\n",
    "        scores = dict()\n",
    "        metrics = ['Precision', 'Recall', 'F1']\n",
    "        models = ['m1', 'm2', 'm3', 'm4']\n",
    "        for ii, metric in enumerate(metrics):\n",
    "                scores[metric] = dict()\n",
    "                for jj, model in enumerate(models):\n",
    "                        scores[metric][model] = dict()\n",
    "                        for zz, abnomality in enumerate(abnormalities):\n",
    "                                scores[metric][model][abnomality] = np.array([])\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "#TODO: Add specificity and add the ability to pull these for each class, maybe can just make an index and label encoder for the metrics\n",
    "#with each numpy array being 2d\n",
    "def Find_metrics(scores: dict, model_name: str, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \n",
    "        def _findProperScore(metric, y_true, y_pred):\n",
    "                if metric == 'Precision':\n",
    "                        return precision_score(y_true, y_pred)  \n",
    "                elif metric == 'Recall':\n",
    "                        return recall_score(y_true, y_pred)\n",
    "                else:\n",
    "                        return f1_score(y_true, y_pred)\n",
    "\n",
    "        for jj, metric in enumerate(scores.keys()):\n",
    "                for ii, cardio_class in enumerate(scores[metric][model_name].keys()):\n",
    "\n",
    "                        scores[metric][model_name][cardio_class] = np.append(scores[metric][model_name][cardio_class], \\\n",
    "                                _findProperScore(metric, y_true[:,ii], y_pred[:,ii]))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Training model 2----------------------\n",
      "\n",
      "-------------------Testing model 1----------------------\n",
      "31/31 [==============================] - 20s 652ms/step\n",
      "\n",
      "-------------------Testing model 2----------------------\n",
      "\n",
      "-------------------Training model 2----------------------\n",
      "\n",
      "-------------------Testing model 1----------------------\n",
      "31/31 [==============================] - 20s 631ms/step\n",
      "\n",
      "-------------------Testing model 2----------------------\n",
      "\n",
      "-------------------Training model 2----------------------\n",
      "\n",
      "-------------------Testing model 1----------------------\n",
      "31/31 [==============================] - 20s 638ms/step\n",
      "\n",
      "-------------------Testing model 2----------------------\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "scores = Score_initator()\n",
    "\n",
    "#Initilaize the models that need to be trained\n",
    "model_verbose = 1\n",
    "model_2 = MyCNN(loss, opt, verbose = model_verbose)\n",
    "'''model_3 = RF_Model(verbose = model_verbose)\n",
    "model_4 = LR_model(verbose = model_verbose)'''\n",
    "\n",
    "#PCA initlization\n",
    "'''PCA_transformer = PCA_Transform(r = 60)'''\n",
    "\n",
    "#TODO: Actually make a validation set for the early stopping\n",
    "#Also there is no regularization in the layers so maybe can add that\n",
    "for train_index, test_index in kf.split(X = tracings_train[:,1,1], y = labels_train):\n",
    "\n",
    "        X_train, X_test = tracings_train[train_index,:,:], tracings_train[test_index,:,:]\n",
    "        y_train, y_test = labels_train[train_index], labels_train[test_index]\n",
    "\n",
    "        #Put data in sequence for models 1 and 2 (CNN)\n",
    "        train_seq = ECGSequence.get_seq(X_train, y_train, batch_size=64)\n",
    "\n",
    "        test_seq = ECGSequence.get_seq(X_test, y_test, batch_size=64)\n",
    "\n",
    "        #Transform data with PCA for models 3 and 4\n",
    "        #TODO: Fix to do for each channel seperatley\n",
    "        '''PCA_transformer.FitData(X_train)\n",
    "        PCA_X_train, PCA_X_test = PCA_transformer.TransformData(X_train, X_test)'''\n",
    "\n",
    "        #Train models\n",
    "        #Here the validation seq is just the test seq since we are using a k-fold analysis\n",
    "        print('\\n-------------------Training model 2----------------------')\n",
    "        #model_2.train(train_seq, val_seq=test_seq)\n",
    "        '''print('\\n-------------------Training model 3----------------------')\n",
    "        model_3.train(X = PCA_X_train, y = y_train)\n",
    "        print('\\n-------------------Training model 4----------------------')\n",
    "        model_4.train(X = PCA_X_train, y = y_train)'''\n",
    "\n",
    "        #Test models - have to round the outputs of the script (TODO check what they do)\n",
    "        print('\\n-------------------Testing model 1----------------------')\n",
    "        model_1_predict = np.round(model_1.predict(test_seq, verbose=model_verbose)).astype(int)\n",
    "        print('\\n-------------------Testing model 2----------------------')\n",
    "        #model_2_predict = np.rint(model_2.predict(test_seq), dtype=int)\n",
    "        '''print('\\n-------------------Testing model 3----------------------')\n",
    "        model_3_predict = model_3.predict(PCA_X_train)\n",
    "        print('\\n-------------------Testing model 4----------------------')\n",
    "        model_4_predict = model_4.predict(PCA_X_train)'''\n",
    "\n",
    "        \n",
    "\n",
    "        #Find metrics\n",
    "        #Call the function each time and then can average for each class after\n",
    "\n",
    "        scores = Find_metrics(scores, 'm1', y_test, model_1_predict)\n",
    "        #scores = Find_metrics(scores, 'm2', y_test, model_2_predict)\n",
    "        '''scores = Find_metrics(scores, 'm3', y_test, model_3_predict)\n",
    "        scores = Find_metrics(scores, 'm4', y_test, model_3_predict)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Models**\n",
    "\n",
    "Train all the models that need to be trained on the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = ECGSequence.get_seq(tracings_train, labels_train, batch_size=64)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Models**\n",
    "\n",
    "Test all the models with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Testing model 1----------------------\n",
      "13/13 [==============================] - 8s 616ms/step\n",
      "\n",
      "-------------------Testing model 2----------------------\n",
      "{'Precision': {'m1': {'1dAVb': array([0.88888889]), 'RBBB': array([0.91891892]), 'LBBB': array([1.]), 'SB': array([0.78947368]), 'AF': array([0.84615385]), 'ST': array([0.92307692])}, 'm2': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm3': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm4': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}}, 'Recall': {'m1': {'1dAVb': array([0.85714286]), 'RBBB': array([1.]), 'LBBB': array([1.]), 'SB': array([0.9375]), 'AF': array([0.84615385]), 'ST': array([0.97297297])}, 'm2': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm3': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm4': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}}, 'F1': {'m1': {'1dAVb': array([0.87272727]), 'RBBB': array([0.95774648]), 'LBBB': array([1.]), 'SB': array([0.85714286]), 'AF': array([0.84615385]), 'ST': array([0.94736842])}, 'm2': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm3': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}, 'm4': {'1dAVb': array([], dtype=float64), 'RBBB': array([], dtype=float64), 'LBBB': array([], dtype=float64), 'SB': array([], dtype=float64), 'AF': array([], dtype=float64), 'ST': array([], dtype=float64)}}}\n"
     ]
    }
   ],
   "source": [
    "Test_scores = Score_initator()\n",
    "test_seq = ECGSequence.get_seq(tracings_test, labels_test, batch_size=64)\n",
    "\n",
    "#Predict with the models\n",
    "print('\\n-------------------Testing model 1----------------------')\n",
    "model_1_predict = model_1.predict(test_seq, verbose=1)\n",
    "optimal_thresholds = FindThresholds(model_1_predict, labels_test)\n",
    "model_1_predict = ProcessOutputs(model_1_predict, optimal_thresholds)\n",
    "print('\\n-------------------Testing model 2----------------------')\n",
    "#model_2_predict = np.rint(model_2.predict(test_seq), dtype=int)\n",
    "'''print('\\n-------------------Testing model 3----------------------')\n",
    "model_3_predict = model_3.predict(PCA_X_train)\n",
    "print('\\n-------------------Testing model 4----------------------')\n",
    "model_4_predict = model_4.predict(PCA_X_train)'''\n",
    "\n",
    "\n",
    "Test_scores = Find_metrics(Test_scores, 'm1', labels_test, model_1_predict)\n",
    "print(Test_scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
